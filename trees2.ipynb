{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd484f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import subplots\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ISLP import load_data, confusion_table\n",
    "from ISLP.models import ModelSpec as MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fa7a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import (DecisionTreeClassifier as DTC,\n",
    "                          DecisionTreeRegressor as DTR,\n",
    "                          plot_tree,\n",
    "                          export_text)\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             log_loss)\n",
    "from sklearn.ensemble import \\\n",
    "     (RandomForestRegressor as RF,\n",
    "      GradientBoostingRegressor as GBR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7560b9be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "Auto = load_data('Auto')\n",
    "Auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f558da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also impute the missing values of \"horsepower\" as we did previously\n",
    "\n",
    "Auto['horsepower'].replace('?','104',inplace=True)\n",
    "Auto['horsepower'] = pd.to_numeric(Auto['horsepower'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef57a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the numeric variables in the data to predict mpg\n",
    "\n",
    "X = Auto[['cylinders','displacement','horsepower','weight','acceleration','year','origin']]\n",
    "y = Auto['mpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb79ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state=314,\n",
    "                                                    test_size=0.25,\n",
    "                                                    shuffle=True) \n",
    "Train = pd.merge_ordered(X_train,y_train,left_on=X_train.index,right_on=y_train.index).drop(columns=['key_0'])\n",
    "Test = pd.merge_ordered(X_test,y_test,left_on=X_test.index,right_on=y_test.index).drop(columns=['key_0'])\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad19d0f",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a0a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "# Documentation: https://scikit-learn.org/dev/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\n",
    "\n",
    "gbm_auto = GBR(learning_rate=0.001,\n",
    "               n_estimators=5000,\n",
    "               max_depth=5,\n",
    "               random_state=314)\n",
    "gbm_auto.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b1482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE on test set\n",
    "\n",
    "# Get predictions on test\n",
    "y_hat_gbm = gbm_auto.predict(X_test)\n",
    "\n",
    "# Calculate MSE\n",
    "mse_gbm = np.mean((y_test - y_hat_gbm)**2)\n",
    "print('test mse: ',mse_gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881adb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can actually plot the \"progress\" of a GBM\n",
    "\n",
    "test_error = np.zeros_like(gbm_auto.train_score_)\n",
    "for idx, y_ in enumerate(gbm_auto.staged_predict(X_test)):\n",
    "   test_error[idx] = np.mean((y_test - y_)**2)\n",
    "\n",
    "plot_idx = np.arange(gbm_auto.train_score_.shape[0])\n",
    "ax = subplots(figsize=(8,8))[1]\n",
    "ax.plot(plot_idx,\n",
    "        gbm_auto.train_score_,\n",
    "        'b',\n",
    "        label='Training')\n",
    "ax.plot(plot_idx,\n",
    "        test_error,\n",
    "        'r',\n",
    "        label='Test')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63693fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance list\n",
    "feature_names = list(X_train.columns)\n",
    "feature_imp = pd.DataFrame(\n",
    "    {'importance':gbm_auto.feature_importances_},\n",
    "    index=feature_names)\n",
    "feature_imp.sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82e2f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask ChatGPT to create a function to make lift charts\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_regression_lift_chart(y_true, y_preds, n_bins=10):\n",
    "    \"\"\"\n",
    "    Plots a lift chart for a regression model.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: Array-like of true continuous values.\n",
    "    - y_preds: Array-like of predicted values.\n",
    "    - n_bins: Number of bins to use for the lift chart.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame with true values and predicted values\n",
    "    data = pd.DataFrame({'y_true': y_true, 'y_preds': y_preds})\n",
    "    \n",
    "    # Sort the DataFrame by predicted values\n",
    "    data = data.sort_values(by='y_preds', ascending=False)\n",
    "\n",
    "    # Create bins based on predicted values\n",
    "    data['bin'] = pd.qcut(data['y_preds'], n_bins, labels=False)\n",
    "\n",
    "    # Calculate lift\n",
    "    lift_data = data.groupby('bin').agg(\n",
    "        total_count=('y_true', 'count'),\n",
    "        actual_mean=('y_true', 'mean'),\n",
    "        predicted_mean=('y_preds', 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Calculate expected mean (mean of actual values)\n",
    "    expected_mean = data['y_true'].mean()\n",
    "    \n",
    "    # Calculate lift\n",
    "    lift_data['lift'] = lift_data['actual_mean'] / expected_mean\n",
    "\n",
    "    # Plot the lift chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(lift_data['bin'], lift_data['lift'], marker='o', label='Lift')\n",
    "    plt.axhline(y=1, color='r', linestyle='--', label='Random Guessing Lift (1)')\n",
    "    plt.title('Lift Chart for Regression Model')\n",
    "    plt.xlabel('Bins')\n",
    "    plt.ylabel('Lift')\n",
    "    plt.xticks(lift_data['bin'])\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f32f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regression_lift_chart(y_test, y_hat_gbm, n_bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa3b627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
